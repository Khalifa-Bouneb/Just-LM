# Just-LM: Language Modeling from Scratch ğŸ§ ğŸš€

## ğŸ“‹ Introduction

Welcome to **Just-LM**, a personal deep dive into **Language Modeling (LM)**.
This repository documents my journey through **Stanford CS336: Language Modeling**, focusing on building models and ideas **from scratch** using **PyTorch**.

The goal is to reproduce key techniques from influential papers and lectures, gaining both **theoretical understanding** and **hands-on experience** with large language models â€” from simple RNNs to modern Transformers.

---

## ğŸ¯ Progress

**CS336: Language Modeling (Stanford)** â€” A structured exploration of language modeling fundamentals, covering:

* Tokenization & text preprocessing
* Transformer architectures & attention
* Scaling laws, pre-training, and fine-tuning
* Filtering Language Modeling Data
* Alignement and Reasoning RL

### ğŸ§‘â€ğŸ« Lectures

Progress tracker:
ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ (4 / 24 lectures completed)

---

## ğŸ§© Implementations

* [Ã¾] **Assignment 1:** Building a Transformer LM (completed)
* [ ] **Assignment 2:** Systems and Parallelism (up next)
* [ ] **Assignment 3:** Scaling laws
* [ ] **Assignment 4:** Filtering Language Modeling Data
* [ ] **Assignment 5:** Alignement and Reasoning RL

---

## ğŸš€ Beyond CS336: Advanced Language Modeling

Once the course foundation is solid, the plan is to go **beyond the curriculum**, including:

* Implementing Transformer variants (GPT, BERT, RoPE, etc.)
* Exploring efficient training (FlashAttention, LoRA, Quantization)
* Training a small GPT-style model from scratch
* Experimenting with dataset curation, evaluation, and alignment

---

## ğŸ§  Technologies

* **Framework:** PyTorch
* **Environment:** Python 3.10+ / Jupyter Notebooks
* **Tools:** NumPy, Matplotlib, tqdm, tokenizers

---

## ğŸ“˜ Acknowledgments

Course materials and inspiration from:

* [CS336: Language Modeling (Stanford)](https://web.stanford.edu/class/cs336/)
* Foundational LLM papers (GPT, Attention is All You Need, BERT, etc.)

---

*â€œLearning by building â€” one model at a time.â€*
